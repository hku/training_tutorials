{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、问题的定义\n",
    "\n",
    "已知一组随机向量 $\\mathbf{V} = \\{V_1, V_2,\\cdots,V_M\\}$ (其意义不妨想象成一张黑白图片$M$个像素点的灰度值)，各个 $V_i$ 的取值空间(概率论中称 support space )一致，记为$\\Lambda$。 显然 $\\mathbf{V}$ 的support space是 $\\Lambda^M$. 我们认为存在一个概率函数 $p(\\mathbf{v})$ 来描述$\\mathbf{V}$ 的概率分布，即：\n",
    "\n",
    "$$\\mathbf{V}\\sim p(\\mathbf{v}), \\quad\\text{$\\mathbf{v}\\equiv\\{v_1,v_2,\\cdots,v_M\\}\\in\\Lambda^M$ 是 $\\mathbf{V}$ 的某个可取值}$$\n",
    "\n",
    "** 假设我们已知 $T$ 个 $\\mathbf{V}$ 的观测数据 $\\mathbf{v}^{(i)},\\;i=1,2,\\cdots,T$, 怎样 sampling 出类似于给定数据的新样例？**\n",
    "\n",
    "为了处理这个问题，我们** 需要“拟合”$p(\\mathbf{v})$ **。\n",
    "\n",
    "\n",
    "为了“拟合” $p(\\mathbf{v})$，现在我们引入一组新随机变量 $\\mathbf{H}\\equiv\\{H_1, H_2,\\cdots, H_N\\}$ （称为 “hidden variables”或 \"latent variables\", 每个 $H_i$ 的 support space 也是$\\Lambda$，显然 $\\mathbf{H}$ 的support space 是 $\\Lambda^N$），并构造如下“拟合”框架：\n",
    "\n",
    "认为存在一个joint distribution $p(\\mathbf{v,h})$ 描述全部的随机变量 $(\\mathbf{V, H})$，即\n",
    "\n",
    "$$(\\mathbf{V, H})\\sim p(\\mathbf{v, h}),\\quad \\mathbf{v}\\equiv\\{v_1,v_2\\cdots,v_M\\}\\in\\Lambda^M,\\;\\mathbf{h}\\equiv\\{h_1,h_2,\\cdots h_N\\}\\in\\Lambda^N$$\n",
    "\n",
    "\n",
    "而 ** $p(\\mathbf{v})$ 将由$p(\\mathbf{v})\\equiv\\sum\\limits_\\mathbf{h}p(\\mathbf{v,h})$ 给出 **。（当然在该框架中我们也可以定义hidden variables 的分布 $p(\\mathbf{h})\\equiv\\sum\\limits_\\mathbf{v}p(\\mathbf{v,h})$）\n",
    "\n",
    "\n",
    "为了后文叙述方便，我们将 $\\mathbf{v}$ 和 $\\mathbf{h}$ 统一记为 $\\mathbf{\\sigma\\equiv(v, h)}$，于是 $p(\\mathbf{v, h})\\equiv p(\\mathbf{\\sigma})$\n",
    "\n",
    "对于$p(\\mathbf{\\sigma})$，我们总可以构造相应的“能量”函数$E(\\mathbf{\\sigma})$ 满足：\n",
    "\n",
    "$$p(\\mathbf{\\sigma})\\equiv {1\\over Z}\\exp[-E(\\mathbf{\\sigma})]$$\n",
    "\n",
    "其中归一化系数（物理中称配分函数）定义为：\n",
    "\n",
    "$$Z\\equiv\\sum_\\mathbf{\\sigma}\\exp[-E(\\mathbf{\\sigma})]$$\n",
    "\n",
    "> 下文推导中常用的一个公式：\n",
    "\n",
    "> 设有$N$个随机变量 $\\{X_1,X_2,\\cdots,X_N\\}$, $s_i\\equiv\\sum\\limits_{X_i}{f(X_i)}$ 表示对$X_i$ 所有“可取值”的求和。(注意给定函数$f$后，这个表达式可以看做为一个<strong>依赖于指标$i$</strong>的函数,即$s_i$)，则\n",
    "\n",
    "> $$\\prod_{i=1}^N\\left(\\sum\\limits_{X_i}f(X_i)\\right)\\equiv \\sum\\limits_{X_1,X_2\\dots,X_N}f(X_1)f(X_2)\\cdots f(X_N)=\\sum\\limits_{X_1,X_2\\dots,X_N}\\left(\\prod_{i=1}^N f(X_i)\\right)$$\n",
    "\n",
    "> 物理意义上可概括为：“对因子的每个局部configuration遍历求和后连乘，等于因子连乘后对整体configuration的遍历求和”\n",
    "\n",
    "> 此外若一个函数 $f$ 依赖于 $N$ 个变量  $\\{X_1,X_2,\\cdots,X_N\\}$，我们将对函数所有参数在各自support space的遍历求和称为“trace”，即\n",
    "\n",
    "> $${\\rm Tr}\\; f(X_1,X_2,\\cdots,X_N) \\equiv \\sum\\limits_{X_1,X_2\\dots,X_N} f(X_1,X_2,\\cdots,X_N)$$\n",
    "\n",
    "\n",
    "## 二、定义 Boltzmann Machine\n",
    "\n",
    "在 Boltzmann Machine 框架中，我们约定 $E(\\sigma)$ 可以由一个类似下图所示的 graph $\\mathcal{G}$ 来描述, 其中每个node $i$ 对应一个随机变量$\\Sigma_i$,（取值记为 $\\sigma_i$），每个node贡献一个能量因子 $-b_i\\sigma_i$, 每个edge贡献一个能量因子 $-w_{ij}\\sigma_i\\sigma_j$.\n",
    "\n",
    "> graph $\\mathcal{G}$ 定义为 $\\mathcal{G\\equiv (V, E)}$，其中 $\\mathcal{V}$ 表示 node (vertex) 的集合， $\\mathcal{E}$ 表示edge的集合。为后文叙述方便，$\\forall i$ 的neighborhood 记为 $\\mathcal{N}_i$， $\\mathcal{N}_i\\equiv \\{j:\\;\\langle ij\\rangle \\in \\mathcal{E}\\}$，$\\overline{\\mathcal{N}}_i\\equiv \\{i\\}\\cup \\mathcal{N}_i$\n",
    "\n",
    "![BM](../images/BM.jpg)\n",
    "\n",
    "即，$$p(\\mathbf{\\sigma})\\equiv {1\\over Z}\\exp[-E(\\mathbf{\\sigma})],\\quad E(\\mathbf{\\sigma})=-\\sum_{\\langle ij\\rangle} w_{ij}\\sigma_i\\sigma_j - \\sum_i b_i\\sigma_i,\\quad\\text{其中 $\\sum\\limits_{\\langle ij\\rangle}$ 表示对所有edge 求和}$$\n",
    "\n",
    "下面我们证明： ** Boltzmann Machine 是定义在 graph $\\mathcal{G}$ 上的一个MRF (Markov Random Field) ** 。即要证明：给定 Boltzmann Machine 中任意节点 $i$ “neighborhood”的所有 $\\sigma$ 值，则 $\\sigma_i$ 的取值概率被唯一确定。 （More formally, $p\\left(\\sigma_i\\left|\\left\\{\\sigma_j:\\; j\\in\\mathcal{N}_i\\right\\}\\right.\\right)$  is a function of only $\\sigma_i$ and $\\sigma_{j\\in\\mathcal{N}_i}$）\n",
    "\n",
    "\n",
    "> “graph $\\mathcal{G}$ 上的 Random Field”，其含义是: associate a random Variable $\\Sigma_i$ to every node $i$, 所有这些 random Variable 构成的集合称为 Random Field $\\mathbf{\\Sigma}$\n",
    "\n",
    "> 当 $\\mathbf{\\Sigma}$ 对应的 joint distribution $\\mathbf{\\Sigma} \\sim p(\\mathbf{\\sigma})$ 使如下条件满足时，称$\\mathbf{\\Sigma}$ 为 ** Markov Random Field **，该条件又被称为 local Markov property (严格定义详见：[RBM.pdf](../papers/RBM.pdf))： $\\forall i:\\;\\Sigma_i$ is conditional independent from any other variables, if all the values of $\\{\\Sigma_j:\\;j\\in\\mathcal{N}_i\\}$ are given  \n",
    "\n",
    "选取 Boltzmann Machine 中任意 node $i$, 我们总可以将“能量”函数写成如下形式：\n",
    "$$E(\\mathbf{\\sigma}) = E(\\mathbf{\\sigma}\\backslash\\sigma_i) - \\sum\\limits_{j\\in \\mathcal{N}_i}w_{ij}\\sigma_j\\sigma_i - b_i\\sigma_i, \\quad\\text{\n",
    "其中 $E(\\mathbf{\\sigma}\\backslash\\sigma_i)$ 是$E(\\mathbf{\\sigma})$ 中不包含 $\\sigma_i$ 的部分}$$\n",
    "\n",
    "引入记号 $\\mathcal{N}(\\sigma_i)\\equiv \\left\\{\\sigma_j:\\; j\\in \\mathcal{N}_i\\right\\},\\quad \\overline{\\mathcal{N}}(\\sigma_i)\\equiv \\left\\{\\sigma_i\\right\\}\\cup\\left\\{\\sigma_j:\\; j\\in \\mathcal{N}_i\\right\\}$\n",
    "\n",
    "\n",
    " $$\n",
    " \\begin{align}\n",
    " p\\left(\\sigma_i\\left|\\left\\{\\sigma_j:\\; j\\in\\mathcal{N}_i\\right\\}\\right.\\right) &=\n",
    " {p\\left(\\sigma_i,\\left\\{\\sigma_j:\\; j\\in \\mathcal{N}_i\\right\\}\\right)\\over \n",
    " p\\left(\\left\\{\\sigma_j:\\; j\\in \\mathcal{N}_i\\right\\}\\right)}= {\\sum\\limits_{\\mathbf{\\sigma}\\backslash  \\overline{\\mathcal{N}}(\\sigma_i) }p(\\sigma)\\over \\sum\\limits_{\\mathbf{\\sigma}\\backslash \\mathcal{N}(\\sigma_i) }p(\\sigma)}\\\\\n",
    " &= \\left.\\left(\n",
    " \\sum\\limits_{\\mathbf{\\sigma}\\backslash  \\overline{\\mathcal{N}}(\\sigma_i) } \\exp\\left[\n",
    " -E(\\mathbf{\\sigma}\\backslash\\sigma_i) + \\sum\\limits_{j\\in \\mathcal{N}_i}w_{ij}\\sigma_j\\sigma_i + b_i\\sigma_i\n",
    " \\right]\n",
    " \\right)\\right/\\left(\n",
    " \\sum\\limits_{\\mathbf{\\sigma}\\backslash  \\mathcal{N}(\\sigma_i) } \\exp\\left[\n",
    " -E(\\mathbf{\\sigma}\\backslash\\sigma_i) + \\sum\\limits_{j\\in \\mathcal{N}_i}w_{ij}\\sigma_j\\sigma_i + b_i\\sigma_i\n",
    " \\right]\n",
    " \\right)\\\\\n",
    " &= {\\exp\\left(z_i\\sigma_i\\right)\\over\\sum\\limits_{\\sigma_i}\\exp\\left(z_i\\sigma_i\\right)},\\quad\\text{其中 $z_i= \\sum\\limits_{j\\in \\mathcal{N}_i}w_{ij}\\sigma_j + b_i$}&\\text{（2.1）}\n",
    " \\end{align}\n",
    " $$\n",
    " \n",
    " 该表达式仅依赖与 $\\overline{\\mathcal{N}}(\\sigma_i)$, 因此 Bolztmann Machine 是定义在 graph $\\mathcal{G}$ 上的 MRF\n",
    "\n",
    "\n",
    "## 三、 Gibbs sampling\n",
    "\n",
    "\n",
    "设 $f(\\mathbf{\\Sigma})$ 是定义在 Markov Random Field $\\mathbf{\\Sigma\\equiv(V, H)}$ 上的某个量 , 其期望值为\n",
    "\n",
    "$$ \\langle f\\rangle = \\sum\\limits_{\\sigma} p(\\sigma) f(\\sigma)$$\n",
    "\n",
    "实际的数值计算中，我们往往需要通过sampling的方式取 $T$ 个 $\\mathbf{\\Sigma}$ 的样例来计算上述求和，即\n",
    "\n",
    "$$\\langle f\\rangle \\approx {1\\over T} \\sum\\limits_{\\alpha=1}^Tf\\left(\\mathbf{\\sigma}^{(\\alpha)}\\right), \\quad\\text{其中 $\\left\\{\\mathbf{\\sigma}^{(\\alpha)}\\right\\}$ 是根据 $p(\\mathbf{\\sigma})$ 随机采样的$T$个样本} $$\n",
    "\n",
    "由于 $\\mathbf{\\Sigma}$ 是多个耦合随机变量的集合，因此怎样 sampling $\\mathbf{\\Sigma}$ 不是一件 trivial的事情。 对于 MRF 我们可以采用  <span style=\"color:red\">MCMC (Markov Chain Monte Carlo)</span> 的方式来采样，其具体步骤如下：\n",
    "<strong>\n",
    "1. 按照任意方式，选取一组 $\\Sigma$ 的初始值，记为$\\mathbf{\\sigma}^{(0)}=\\left\\{\\sigma^{(0)}_1,\\;\\sigma^{(0)}_2,\\;\\sigma^{(0)}_3\\cdots\\right\\}$ 每个分量和graph中的一个node对应\n",
    "\n",
    "2. 按一定规则选定 graph 中的某个 node $i$ (可以按确定的概率分布选取，也可以轮循遍历)\n",
    "\n",
    "3. 根据 node $i$ 周边的 $\\sigma^{(0)}$ 值，按 $p\\left(\\sigma_i\\left|\\left\\{\\sigma_j:\\; j\\in\\mathcal{N}_i\\right\\}\\right.\\right)$，sampling 一个新的 $\\sigma_i$ 值。 （当 $p\\left(\\sigma_i\\left|\\left\\{\\sigma_j:\\; j\\in\\mathcal{N}_i\\right\\}\\right.\\right)$ 取 公式 （2.1） 时，称为 <span style=\"color:red\">Gibbs sampling</span>）\n",
    "\n",
    "4. goto step 2， 如此反复很多次 (此时 $\\sigma$ 的值，已可以看做是按 $p(\\sigma)$ sampling 产生的)\n",
    "\n",
    "5. 继续重复这种步骤，产生足够多的 sample\n",
    "</strong>\n",
    "\n",
    "Gibbs sampling 有效性的论证详见 [RBM.pdf](../papers/RBM.pdf) ，其基本思想是：构造一个Markov chain，其对应的 stational distribution 恰是 $p(\\sigma)$\n",
    "\n",
    "\n",
    "> Given a time series $\\mathcal{X}=\\left[\\mathbf{X}^{(0)},\\mathbf{X}^{(1)},\\mathbf{X}^{(2)},\\cdots\\right]$, with every random variable $\\mathbf{X}^{(i)}$ take values in a set $\\Omega$. (注意：$\\mathbf{X}^{(i)}$ 可能是多维张量) 引入以下定义：\n",
    "\n",
    "> * $\\mathcal{X}$ is a <strong>Markov chain</strong> if distribution of $\\mathbf{X}^{(k+1)}$ only depends on the value of $\\mathbf{X}^{(k)}$，即 $p\\left(\\mathbf{X}^{(k+1)}\\;\\right|\\left. \\mathbf{X}^{(k)}\\;,\\mathbf{X}^{(k-1)}\\;,\\cdots\\mathbf{X}^{(0)}\\;\\right)\\equiv p\\left(\\mathbf{X}^{(k+1)}\\;\\right|\\left. \\mathbf{X}^{(k)}\\;\\right)$\n",
    "\n",
    "> Suppose $\\Omega$ is finite (直观上即\"countable\"), i.e., at step $k$, the conditional distribution $p^{(k)}_{ij}\\equiv p\\left(\\mathbf{X}^{(k+1)}=j\\;\\right|\\left. \\mathbf{X}^{(k)}=i\\;\\right)$,可以按其脚标被看做“矩阵”$\\mathbf{P}^{(k)}$（称为 <strong>transition matrix</strong>）的元素. \n",
    "\n",
    "> * $\\mathcal{X}$  is a <strong>\"homogeneous\" Markov chain</strong>, if $p^{(k)}_{ij}$ is independent of  $k$，即 $\\mathbf{P} \\equiv \\mathbf{P}^{(k)}$. 假设step $k$ 的随机变量 $\\mathbf{X}^{(k)}$ 在$\\Omega$ 上的概率分布为$\\mu^{(k)}$ （看做一个行矢量，元素$\\mu^{(k)}_i$对应 概率值 $p\\left(X^{(k)}=i\\right),\\quad i\\in\\Omega$; 则显然 $\\mu^{(k+1)}=\\mu^{(k)}P$。 给定transition matrix $\\mathbf{P}$, 定义在$\\Omega$上的概率分布$\\pi$, 称为<strong>stational distribution</strong>, if $\\pi=\\pi\\mathbf{P}$ （即 $\\pi_i=\\sum\\limits_j\\pi_jp_{ji}$ 这个关系也称为balance condition）。若 $\\pi$ 进一步满足 $\\pi_ip_{ij}=\\pi_jp_{ji}$时，称$\\pi$ 是 detailed balanced 的。由于$p_{ij}$ 的归一性关系，显然若$\\pi$满足 detailed balance condition, $\\pi$ 也一定满足 balance condition\n",
    "\n",
    "\n",
    "> *  a homogeneous Markov chain $\\mathcal{X}$ 称为 <strong>“irreducible”</strong>, 如果对于$\\forall i,j\\in \\Omega$，在有限长的步数内，总有可能从样本点$i$ 跳到样本点$j$, more formally, $\\forall i,j\\in\\Omega$, $\\exists k>0$ with $p\\left(X^{(k)}=j\\right|\\left.X^{(0)}=i\\right)>0$.  (给定$\\Omega$，irreducible性可以看作是对$\\mathbf{P}$某种内在<span style=\"color:red\">连通性</span>、关联性的约束)。\n",
    "有定理：<strong>对于“irreducible” Markov chain，存在<span style=\"color:red\">唯一</span>的stational distribution</strong>\n",
    "\n",
    "\n",
    "> * a homogeneous Markov chain $\\mathcal{X}$ 称为 <strong>“aperiodic”</strong>, 如果从$\\forall i\\in \\Omega$出发，采样结果再跳回$i$的所有可能步数值不可约，more formally, if $\\forall i\\in\\Omega$, the greatest common divisor of $\\left\\{k\\;:\\; p\\left(X^{(k)}=i\\right|\\left.X^{(0)}=i\\right)>0\\right\\}$ is 1. (给定$\\Omega$，aperiodic性可以看做是对$\\mathbf{P}$某种内在<span style=\"color:red\">自斥性</span>、非周期性的约束，想象“贪吃蛇”不允许碰到自己)。\n",
    "\n",
    "> 定理：<strong>对于连通性、自斥性足够好的 homogeneous Markov chain，从任意分布 $\\mu$ 出发，经过足够长步数迭代，结果总趋向于该Markov chain唯一的stational distribution $\\pi$.</strong> more formally:\n",
    "\n",
    "> For a irreducible aperiodic homogeneous Markov chain, we have  $\\forall \\mu,\\;\\mu P^k \\xrightarrow{k\\rightarrow\\infty}\\pi$\n",
    "\n",
    "\n",
    "## 四、 RG 过程\n",
    "\n",
    "RG变换 通过如下公式定义：\n",
    "\n",
    "$$\\exp[-\\tilde{E}_\\theta(\\mathbf{s})] \\equiv \\sum_\\mathbf{\\sigma} K_\\theta(\\mathbf{s,\\sigma}) \\exp\\left[-E(\\mathbf{\\sigma})\\right]$$\n",
    "\n",
    "其中 $K_\\theta(\\mathbf{s,\\sigma}) $ 称为 kernel, $\\tilde{E}_\\theta(\\mathbf{s})$ 称为 \"renormalized energy\", $\\mathbf{s}$ 是一组新变量，$\\theta$ 是该变换依所依赖的parameter。\n",
    "\n",
    "定义 \"renormalized partial function\" 为：\n",
    "$$\\tilde{Z}_\\theta\\equiv \\sum\\limits_\\mathbf{s}\\exp[-\\tilde{E}_\\theta(\\mathbf{s})] $$\n",
    "\n",
    "RG变换中，参数 $\\theta$ 选取的原则是 minimize $\\left\\|\\tilde{Z}_\\theta - Z\\right\\|$, 特别地，当$K$ 恰满足如下归一性条件时，\n",
    "$$\\forall \\mathbf{\\sigma}:\\;\\sum\\limits_\\mathbf{s}K(\\mathbf{s,\\sigma}) = 1 $$\n",
    "\n",
    "我们有\n",
    "\n",
    "$$\\left\\|\\tilde{Z} - Z\\right\\| =\\left\\|\\sum\\limits_\\mathbf{s}\\sum_\\mathbf{\\sigma} K(\\mathbf{s,\\sigma}) \\exp\\left[-E(\\mathbf{\\sigma})\\right] - \\sum_\\mathbf{\\sigma}\\exp[-E(\\mathbf{\\sigma})]\\right\\|= \\left\\|\\sum_\\mathbf{\\sigma}\\exp[-E(\\mathbf{\\sigma})] - \\sum_\\mathbf{\\sigma}\\exp[-E(\\mathbf{\\sigma})]\\right\\|=0$$\n",
    "\n",
    "即此时 $K$ 是一个严格 \"energy-reserved\" 变换，因此完美地实现了minimize $\\left\\|\\tilde{Z}_\\theta - Z\\right\\|$\n",
    "\n",
    "RG 的核心思想是：<strong>一个物理系统存在一些fixed points，其中stable fixed point对应该系统的某个“理想相态”（如“理想气体”，“理想液体”等），unstable fixed point 对应该系统的“相变点”；（这里的所谓“fixed” 是相对于RG变换来说的，因此，隐含约定了，在“理想相态”或“相变点”存在RG变换不变性，即 $\\tilde{E}(\\mathbf{s})$ 和 $E(\\mathbf{\\sigma})$ 保持不变）。在某stable fixed point “吸引域”的其他点，会在RG变换下不断向该fixed point逼近，并说明该点也对应这一fixed point所属的“相”（只不过是“非理想相态”而已）</strong>\n",
    "\n",
    "> some comments:\n",
    "\n",
    "> continue changes in length scale until we reach limits of system (finite system) or\n",
    "\n",
    "> continue changes in length scale until we reach a situation in which coupling change no more (infinite system)\n",
    "\n",
    "> The latter is called a fixed point and describes phases There are three kinds fixed points:\n",
    " \n",
    "> * strong coupling: K,h go to infinity describes e.g. liquid phase\n",
    " \n",
    "> * weak coupling: K, h go to zero describes e.g. vapor phase\n",
    "\n",
    "> * critical: K set to Kc h set to zero, critical point\n",
    " \n",
    "> The different in destinations encode different behavior.Different symmetries and spatial dimensions produce different fixed points. \n",
    "\n",
    "\n",
    "> start with ensemble calculate new ensemble. after many renormalizations, find fixed point:\n",
    "\n",
    "> * at weak coupling fixed point: find averages\n",
    "\n",
    "> * at critical fixed point: find scalings\n",
    "\n",
    "> * at strong coupling fixed point: find theory of nontrivial\n",
    "\n",
    "> behavior, e.g. elasticity, acoustics, ferromagnetism, superconductor. Connect theories on different length scales\n",
    "\n",
    "> There are many RG's. The goal is not to see how many don't work, but rather to find one that does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、定义 RBM\n",
    "\n",
    "对 Boltzmann Machine 做如下约束，仅允许 visible nodes 和 hidden nodes 有“连线”，visible nodes 彼此之间无“连线”，hidden nodes 之间也无“连线”；这种特殊情形称为：RBM (Restricted Boltzmann Machine)。 即， RBM 的能量函数可以写作：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E(\\mathbf{v, h})\\equiv -\\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^M w_{ij} h_iv_j - \\sum\\limits_{j=1}^Mb_jv_j- \\sum\\limits_{i=1}^Nc_ih_i\\qquad\\text{(5.1)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathbf{v}) \\equiv \\sum_\\mathbf{h}p(\\mathbf{v,h})={1\\over Z}\\sum_\\mathbf{h} e^{-E(\\mathbf{v,h})}& =\n",
    "{1\\over Z}\\left(\\exp\\left[\\sum_{j=1}^Mb_jv_j\\right]\\right)\\sum_{h_1\\;h_2\\cdots h_N}\\left(\\prod_{i=1}^N\\exp\\left[h_i\\left(c_i + \\sum_{j=1}^Mw_{ij}v_j\\right)\\right]\\right)\\\\\n",
    "& = {1\\over Z} \\left(\\prod_{j=1}^M \\exp\\left[b_jv_j\\right]\\right)\\prod_{i=1}^N\\left(\\sum_{h_i}\\exp\\left[h_i\\left(c_i + \\sum_{j=1}^Mw_{ij}v_j\\right)\\right]\\right)\\qquad\\text{(5.2)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "同样我们有：\n",
    "$$\n",
    "p(\\mathbf{h}) \\equiv \\sum_\\mathbf{v}p(\\mathbf{v,h})={1\\over Z} \\left(\\prod_{i=1}^N \\exp\\left[c_ih_i\\right]\\right)\\prod_{j=1}^M\\left(\\sum_{v_j}\\exp\\left[v_j\\left(b_j + \\sum_{i=1}^Nw_{ij}h_i\\right)\\right]\\right)\\qquad\\text{(5.3)}\n",
    "$$\n",
    "\n",
    "条件概率：\n",
    "$$\n",
    "\\begin{align}\n",
    "p(h_i|\\mathbf{v}) \\equiv {p(h_i,\\mathbf{v})\\over p(\\mathbf{v})} = {\\sum\\limits_{\\mathbf{h}\\backslash h_i}p(\\mathbf{v,h})\\over \\sum\\limits_\\mathbf{h}p(\\mathbf{v,h})}\n",
    "\\end{align}\n",
    "$$\n",
    "其中 $\\sum\\limits_{\\mathbf{h}\\backslash h_i}$ 表示对$\\mathbf{h}=\\{h_1,h_2,\\cdots,h_N\\}$ 中 ** 除 $h_i$ 外 ** 的所有变量求和，由于\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum\\limits_{\\mathbf{h}\\backslash h_i}p(\\mathbf{v,h})=\n",
    "{1\\over Z} \\exp\\left[h_i\\left(c_i + \\sum_{j=1}^Mw_{ij}v_j\\right)\\right] \\left(\\prod_{j=1}^M \\exp\\left[b_jv_j\\right]\\right)\\prod_{l\\ne i}\\left(\\sum_{h_l}\\exp\\left[h_l\\left(c_l + \\sum_{j=1}^Mw_{lj}v_j\\right)\\right]\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "于是我们有：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(h_i|\\mathbf{v})= {e^{h_iz_i}\\over \\sum\\limits_{h_i}e^{h_iz_i}},\\quad z_i\\equiv c_i + \\sum_{j=1}^Mw_{ij}v_j\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "当 $h_i$ 的取值空间为 $\\{\\pm1\\}$, 于是我们有\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(h_i=1|\\mathbf{v})&= {e^{z_i}\\over e^{z_i} + e^{-z_i}},\\quad p(h_i=-1|\\mathbf{v})= {e^{-z_i}\\over e^{z_i} + e^{-z_i}}\\\\\n",
    "\\mathbb{E}_\\mathbf{v}(h_i)&\\equiv \\sum\\limits_{h_i=\\pm1} h_i p(h_i|\\mathbf{v}) = \\sinh(z_i)\n",
    "\\end{align}\n",
    "$$\n",
    "即 给定$\\mathbf{v}$ 后，$h_i$ 的期望值是 $\\sinh(z_i),\\;z_i\\equiv c_i + \\sum\\limits_{j=1}^Mw_{ij}v_j$。\n",
    "\n",
    "当 $h_i$ 的取值空间为 $\\{0,1\\}$ 时, 不难验证：给定$\\mathbf{v}$ 后，$h_i$ 的期望值是 $\\sigma(z_i)\\equiv {1/(1+ e^{-z_i})}$, 即\n",
    "$$\n",
    "\\mathbb{E}_\\mathbf{v}(h_i)= \\left\\{\n",
    "\\begin{matrix}\n",
    "\\sinh\\left(c_i + \\sum\\limits_{j=1}^Mw_{ij}v_j\\right),\\quad\\text{当 $\\mathbf{h}$ 取值空间为 $\\{\\pm1\\}$时}\\\\\n",
    "\\sigma\\left(c_i + \\sum\\limits_{j=1}^Mw_{ij}v_j\\right),\\quad\\text{当 $\\mathbf{h}$ 取值空间为 $\\{0,1\\}$时}\\\\\n",
    "\\end{matrix}\n",
    "\\right. \\qquad\\text{(5.4)}\n",
    "$$\n",
    "\n",
    "<strong>因此，当我们将RBM看作由$\\mathbf{v}$层向$\\mathbf{h}$层的前馈神经网络时，我们可以将 $\\sinh(z_i)$ 或 $\\sigma(z_i)$ 看作给定输入 $\\mathbf{v}$值后，在$\\mathbf{h}$层的第i个神经元的输出值\n",
    "</strong>\n",
    "\n",
    "同样，不难验证，\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_\\mathbf{h}(v_j)= \\left\\{\n",
    "\\begin{matrix}\n",
    "\\sinh\\left(b_j + \\sum\\limits_{i=1}^Nw_{ij}h_i\\right),\\quad\\text{当 $\\mathbf{v}$ 取值空间为 $\\{\\pm1\\}$时}\\\\\n",
    "\\sigma\\left(b_j + \\sum\\limits_{i=1}^Nw_{ij}h_i\\right),\\quad\\text{当 $\\mathbf{v}$ 取值空间为 $\\{0,1\\}$时}\\\\\n",
    "\\end{matrix}\n",
    "\\right. \\qquad\\text{(5.5)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、Train a RBM\n",
    "\n",
    "结合 RBM 框架，将最初的问题具体描述为： \n",
    "\n",
    "将 RBM 中的 weight ($w_ij$) and $bias$ ($b_i,c_i$) 记为参数 $\\theta$。 已知 $T$ 个 \"visiable\" 观测数据 $\\mathbf{v}^{(t)},\\; t=1,2,\\cdots,T$, 现在的目标是：怎样给出最合理的参数 $\\theta$，使 RBM 可以通过Gibbs sampling 生成类似于给定数据的样例？\n",
    "\n",
    "记数据集$\\mathcal{V}\\equiv\\left\\{\\mathbf{v}^{(1)}, \\mathbf{v}^{(2)},\\cdots, \\mathbf{v}^{(T)}\\right\\}$ ，根据 Bayes 思想，我们有：\n",
    "\n",
    "$$p(\\theta|\\mathcal{V})\\equiv p(\\mathcal{V}|\\theta)p(\\theta),\\quad\\text{$p(\\theta|\\mathcal{V})$ 表示给定数据 $\\mathcal{V}$ 后参数 $\\theta$ 的条件概率分布，$p(\\theta)$ 是参数的先验概率}$$\n",
    "\n",
    "其中$ p(\\mathcal{V}|\\theta)$ 满足 i.i.d.条件： $ p(\\mathcal{V}|\\theta) =\\prod\\limits_{t=1}^T p\\left(\\left.\\mathbf{v}^{(t)}\\right|\\theta\\right)$，因此\n",
    "\n",
    "$$p(\\theta|\\mathcal{V})\\equiv p(\\theta)\\prod\\limits_{t=1}^T p\\left(\\left.\\mathbf{v}^{(t)}\\right|\\theta\\right),\\quad\\text{其中：}\n",
    "p\\left(\\left.\\mathbf{v}^{(t)}\\right|\\theta\\right)\\equiv {1\\over Z_\\theta} \\sum_\\mathbf{h} \\exp\\left[-E_\\theta\\left(\\mathbf{v^{(t)}, h}\\right)\\right],\\quad\\text{$E_\\theta\\left(\\mathbf{v^{(t)}, h}\\right)$ 由公式（5.1）给出} \\qquad \\text{(6.1)}\n",
    "$$\n",
    "\n",
    "注意：** 在 Bayes 思想中，我们只需要知道 $\\theta$ 的分布即可，因此公式(6.1) 已完成我们的目标。** 例如：sampling probability $p(\\mathbf{v})$ 可以由公式 $p(\\mathbf{v}) = \\sum\\limits_{\\theta}p(\\theta|\\mathcal{V})p(\\mathbf{v}|\\theta)$ 给出。 但这个公式涉及到的计算量很大，为此我们进一步根据 saddle point approximation 思想，去寻找极值点 $\\theta^*$ 满足：\n",
    "\n",
    "$$\\theta^*\\equiv \\mathop{\\rm argmax}\\limits_{\\theta} \\; p(\\theta|\\mathcal{V})= \\mathop{\\rm argmax}\\limits_{\\theta} \\;\\log p(\\theta|\\mathcal{V}) = \\mathop{\\rm argmax}\\limits_{\\theta} \\left(\n",
    "{1\\over T}\\log\\;p(\\theta) + {1\\over T}\\sum\\limits_{t=1}^T\\log\\;p\\left(\\left.\\mathbf{v}^{(t)}\\right|\\theta\\right)\\right)$$\n",
    "\n",
    "而对 $p(\\mathbf{v})$ “最合理的”估算，将由 $p\\left(\\mathbf{v}\\left|\\theta^*\\right.\\right)$ 给出\n",
    "\n",
    "当先验概率不依赖$\\theta$（即不考虑 regularization 因子时），我们有：\n",
    "$$\\theta^* = \\mathop{\\rm argmax}\\limits_{\\theta} \\left(\n",
    " {1\\over T}\\sum\\limits_{t=1}^T\\log\\;p\\left(\\left.\\mathbf{v}^{(t)}\\right|\\theta\\right)\\right)\\qquad\\text{(6.2)}$$\n",
    " \n",
    "\n",
    " \n",
    "> 为体现该式子的含义，定义 emprical probability $\\tilde{p}(\\mathbf{v})\\equiv {1\\over T} \\sum\\limits_{t=1}^T\\delta\\left(\\mathbf{v},\\mathbf{v}^{(t)}\\right)$, 于是上式可改写为：\n",
    "\n",
    "> $$\\theta^* = \\mathop{\\rm argmax}\\limits_{\\theta} \\left(\\sum\\limits_\\mathbf{v}\\; \\tilde{p}(\\mathbf{v})\\log p(\\mathbf{v}|\\theta)\\right) =  \\mathop{\\rm argmax}\\limits_{\\theta} {\\rm KL}(\\tilde{p}\\| p_\\theta) \\qquad\\text{(6.3)}$$ \n",
    "\n",
    "\n",
    "\n",
    "> 其中 ** Kullback-Leibler divergence ** 定义为：\n",
    "\n",
    "> $${\\rm KL}(\\tilde{p}\\| p_\\theta) = \\sum\\limits_\\mathbf{v}\\; \\left[\\tilde{p}(\\mathbf{v})\\log p(\\mathbf{v}|\\theta) - \\tilde{p}(\\mathbf{v})\\log \\tilde{p}(\\mathbf{v})\\right]$$\n",
    "\n",
    "为采用 SGD 方法求解 $\\theta^*$, 我们需要求解 $\\left(\n",
    " {1\\over T}\\sum\\limits_{t=1}^T\\log\\;p\\left(\\left.\\mathbf{v}^{(t)}\\right|\\theta\\right)\\right)$ 关于 $\\theta$ 的导数 （形式上等价于 求解 ${\\rm KL}(\\tilde{p}\\| p_\\theta) $ 关于参数 $\\theta$ 的导数），不难证明：\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\partial \\log\\; p(\\mathbf{v}|\\theta)\\over\\partial \\theta} &= {\\partial\\over\\partial\\theta}\\left[\n",
    "\\log\\sum_\\mathbf{h}\\exp\\left[-E(\\mathbf{v,h})\\right] -\\log\\sum\\limits_\\mathbf{v,h}\\exp\\left[-E(\\mathbf{v,h})\\right]\n",
    "\\right]\\\\\n",
    "&=-\\sum\\limits_\\mathbf{h}p(\\mathbf{h|v}) {\\partial E(\\mathbf{v,h})\\over \\partial \\theta} + \\sum\\limits_\\mathbf{v,h}p(\\mathbf{v,h}) {\\partial E(\\mathbf{v,h})\\over \\partial \\theta}\\\\\n",
    "&=-\\left[\\sum\\limits_\\mathbf{h}\\left(p(\\mathbf{h|v}) {\\partial E(\\mathbf{v,h})\\over \\partial \\theta}\\right)\\right] + \\sum\\limits_\\mathbf{v}\\left[p(\\mathbf{v})\\sum\\limits_\\mathbf{h}\\left( p(\\mathbf{h|v}) {\\partial E(\\mathbf{v,h})\\over \\partial \\theta}\\right)\\right],\\quad\\text{其中：} p(\\mathbf{h|v})\\equiv {p(\\mathbf{v,h})\\over p(\\mathbf{v})},\\;\n",
    "p(\\mathbf{v})\\equiv \\sum\\limits_\\mathbf{h}p(\\mathbf{v,h})\\qquad\\text{(6.4)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "因此，\n",
    "\n",
    "$${\\partial {\\rm KL}(\\tilde{p}\\| p_\\theta)\\over\\partial \\theta} = {1\\over T} \\sum\\limits_{t=1}^T \\left\\{ -\\left[\\sum\\limits_\\mathbf{h}\\left(p\\left(\\mathbf{h|v^{(t)}}\\right) {\\partial E\\left(\\mathbf{v^{(t)},h}\\right)\\over \\partial \\theta}\\right)\\right] \\right\\}+ \\sum\\limits_\\mathbf{v}\\left[p(\\mathbf{v})\\sum\\limits_\\mathbf{h}\\left( p(\\mathbf{h|v}) {\\partial E(\\mathbf{v,h})\\over \\partial \\theta}\\right)\\right] \\qquad\\text{(6.5)}$$\n",
    "\n",
    "由于 RBM 的 local Markov property 性质 (给定 $\\mathbf{v}$，hidden variables 之间彼此独立；给定 $\\mathbf{h}$，visible variables 之间彼此独立)，我们有\n",
    "\n",
    "$$ p(\\mathbf{h|v}) = \\prod\\limits_{i=1}^N p(h_i|\\mathbf{v}) $$\n",
    "\n",
    "此外根据定义 (5.1)，能量函数对参数$\\theta$ 的求导可以具象为：\n",
    "\n",
    "$${\\partial E\\left(\\mathbf{v,h}\\right)\\over \\partial w_{ij}} = -h_iv_j,\\;{\\partial E\\left(\\mathbf{v,h}\\right)\\over \\partial c_{i}} = -h_i,\\;{\\partial E\\left(\\mathbf{v,h}\\right)\\over \\partial b_{j}} = -v_j$$\n",
    "\n",
    "因此，公式（6.4）可进一步化为：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\partial \\log\\; p(\\mathbf{v}|\\theta)\\over\\partial w_{ij}} &= \\left[\\sum\\limits_\\mathbf{h}\\left(\\prod_{l=1}^Np({h_l|\\mathbf{v}}) h_iv_j\\right)\\right] -\\sum\\limits_\\mathbf{v}\\left[p(\\mathbf{v})\\sum\\limits_\\mathbf{h}\\left( \\prod_{l=1}^Np({h_l|\\mathbf{v}}) h_iv_j\\right)\\right]\\\\\n",
    "&= \\left(\\sum\\limits_{h_i}p(h_i|\\mathbf{v})h_iv_j\\right)\\prod\\limits_{l\\ne i}\\left(\\sum\\limits_{h_l}p(h_l|\\mathbf{v})\\right)\n",
    "-\\sum\\limits_\\mathbf{v} p(\\mathbf{v})\\left[\\left(\\sum\\limits_{h_i}p(h_i|\\mathbf{v})h_iv_j\\right)\\prod\\limits_{l\\ne i}\\left(\\sum\\limits_{h_l}p(h_l|\\mathbf{v})\\right)\\right]\\\\\n",
    "&=  \\left(\\sum\\limits_{h_i}p(h_i|\\mathbf{v})h_iv_j\\right)\n",
    "-\\sum\\limits_\\mathbf{v}\\left[ p(\\mathbf{v})\\left(\\sum\\limits_{h_i}p(h_i|\\mathbf{v})h_iv_j\\right)\\right]\\\\\n",
    "{\\partial \\log\\; p(\\mathbf{v}|\\theta)\\over\\partial b_{j}} &=\n",
    "v_j\n",
    "-\\sum\\limits_\\mathbf{v}\\left[p(\\mathbf{v})v_j\\right]\\\\\n",
    "{\\partial \\log\\; p(\\mathbf{v}|\\theta)\\over\\partial c_{i}} &=\\left(\\sum\\limits_{h_i}p(h_i|\\mathbf{v})h_i\\right)\n",
    "-\\sum\\limits_\\mathbf{v}\\left[ p(\\mathbf{v})\\left(\\sum\\limits_{h_i}p(h_i|\\mathbf{v})h_i\\right)\\right]\\qquad\\text{(6.6)}\n",
    "\\end{align}\n",
    "$$\n",
    "于是 公式（6.5）可进一步化为：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\partial {\\rm KL}(\\tilde{p}\\| p_\\theta)\\over\\partial  w_{ij}} &= \\left[{1\\over T}\\sum\\limits_{t=1}^T\n",
    "\\left(\\sum\\limits_{h_i}p\\left(h_i|\\mathbf{v}^{(t)}\\right)h_iv_j^{(t)}\\right)\n",
    "\\right]-\\sum\\limits_\\mathbf{v}\\left[ p(\\mathbf{v})\\left(\\sum\\limits_{h_i}p(h_i|\\mathbf{v})h_iv_j\\right)\\right]\\\\\n",
    "{\\partial {\\rm KL}(\\tilde{p}\\| p_\\theta)\\over\\partial b_{j}} &=\n",
    "\\left[{1\\over T}\\sum_{i=1}^Tv_j^{(t)}\\right]\n",
    "-\\sum\\limits_\\mathbf{v}\\left[p(\\mathbf{v})v_j\\right]\\\\\n",
    "{\\partial {\\rm KL}(\\tilde{p}\\| p_\\theta)\\over\\partial c_{i}} &=\\left[{1\\over T}\\sum\\limits_{i=1}^T\\left(\\sum\\limits_{h_i}p\\left(h_i|\\mathbf{v}^{(t)}\\right)h_i\\right)\\right]\n",
    "-\\sum\\limits_\\mathbf{v}\\left[ p(\\mathbf{v})\\left(\\sum\\limits_{h_i}p(h_i|\\mathbf{v})h_i\\right)\\right]\\qquad\\text{(6.7)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> 为体现该式子的含义，定义 emprical probability $\\tilde{p}(\\mathbf{v})\\equiv {1\\over T} \\sum\\limits_{t=1}^T\\delta\\left(\\mathbf{v},\\mathbf{v}^{(t)}\\right),\\; \\tilde{p}(\\mathbf{v,h})\\equiv \\tilde{p}(\\mathbf{v}) p(\\mathbf{h|v})$, 上式可进一步改写为：\n",
    "\n",
    "> $$\n",
    "\\begin{align}\n",
    "{\\partial {\\rm KL}(\\tilde{p}\\| p_\\theta)\\over\\partial  w_{ij}} &= \\left[\\sum\\limits_\\mathbf{v,h}\\tilde{p}(\\mathbf{v,h})h_iv_j\\right]- \\left[\\sum\\limits_\\mathbf{v,h}{p}(\\mathbf{v,h})h_iv_j\\right]\\equiv \\left\\langle h_iv_j\\right\\rangle_{\\rm data} - \\left\\langle h_iv_j\\right\\rangle_{\\rm model}\\\\\n",
    "{\\partial {\\rm KL}(\\tilde{p}\\| p_\\theta)\\over\\partial b_{j}} &=\n",
    "\\left[\\sum\\limits_\\mathbf{v,h}\\tilde{p}(\\mathbf{v,h})v_j\\right]- \\left[\\sum\\limits_\\mathbf{v,h}{p}(\\mathbf{v,h})v_j\\right]\\equiv \\left\\langle v_j\\right\\rangle_{\\rm data} - \\left\\langle v_j\\right\\rangle_{\\rm model}\\\\\n",
    "{\\partial {\\rm KL}(\\tilde{p}\\| p_\\theta)\\over\\partial c_{i}} &=\n",
    "\\left[\\sum\\limits_\\mathbf{v,h}\\tilde{p}(\\mathbf{v,h})h_i\\right]- \\left[\\sum\\limits_\\mathbf{v,h}{p}(\\mathbf{v,h})h_i\\right]\\equiv \\left\\langle h_i\\right\\rangle_{\\rm data} - \\left\\langle h_i\\right\\rangle_{\\rm model}\\qquad\\text{(6.8)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "数值计算中，(6.7)式右侧第一项容易求得，** 第二项涉及“依概率求和”$\\sum\\limits_\\mathbf{v}p(\\mathbf{v})(\\cdots)$，一般需要采用 Gibbs sampling 的方式来采样求近似值 **\n",
    "\n",
    "而 Gibbs sampling过程是很费时的；因此，实际数值计算过程中，$\\theta^*$ 的选取，往往不是通过公式（6.3），而是由下面的公式决定：\n",
    "\n",
    "$$\\theta^* = \\mathop{\\rm argmin}\\limits_{\\theta} CD_k(\\tilde{p},{p}_\\theta)\\equiv\\mathop{\\rm argmin}\\limits_{\\theta} \\left[{\\rm KL}(\\tilde{p}\\| p_\\theta) -{\\rm KL}(\\tilde{p}_k\\| p_\\theta)  \\right] \\qquad\\text{(6.9)}$$\n",
    "\n",
    "其中 $CD_k(\\tilde{p},{p}_\\theta)$ 称为 k-step <strong style=\"color:red\">Contrastive Divergence </strong>， $\\tilde{p}(\\mathbf{v})\\equiv {1\\over T} \\sum\\limits_{t=1}^T\\delta\\left(\\mathbf{v},\\mathbf{v}^{(t)}\\right)$ 是 emprical probability， $\\tilde{p}_k$ 按如下方式定义：\n",
    "\n",
    "把已知的 $T$ 个数据 $\\mathcal{V}\\equiv \\left\\{\\mathbf{v}^{(1)},\\mathbf{v}^{(2)},\\cdots,\\mathbf{v}^{(T)}\\right\\}$ 分别作为初始值， 通过 $k$-step Gibbs sampling （一般 $k$ 值不需要很大，取1或2，就可以了），获得新的 $T$ 个数据 $\\mathcal{U}\\equiv \\left\\{\\mathbf{u}^{(1)},\\mathbf{u}^{(2)},\\cdots,\\mathbf{u}^{(T)}\\right\\}$, 定义$\\tilde{p}_k(\\mathbf{v})$ 为：\n",
    "\n",
    "$$\\tilde{p}_k(\\mathbf{v}) = \\equiv {1\\over T} \\sum\\limits_{t=1}^T\\delta\\left(\\mathbf{v},\\mathbf{u}^{(t)}\\right)$$\n",
    "\n",
    "类似于 (6.5) 式， 我们有\n",
    "\n",
    "$${\\partial {\\rm CD}_k(\\tilde{p}, p_\\theta)\\over\\partial \\theta} = -{1\\over T} \\sum\\limits_{t=1}^T  \\left[\\sum\\limits_\\mathbf{h}\\left(p\\left(\\mathbf{h|v^{(t)}}\\right) {\\partial E\\left(\\mathbf{v^{(t)},h}\\right)\\over \\partial \\theta}\\right)\\right] +{1\\over T} \\sum\\limits_{t=1}^T  \\left[\\sum\\limits_\\mathbf{h}\\left(p\\left(\\mathbf{h|u^{(t)}}\\right) {\\partial E\\left(\\mathbf{u^{(t)},h}\\right)\\over \\partial \\theta}\\right)\\right]\\qquad\\text{(6.10)}$$\n",
    "\n",
    "并类似于（6.7）（6.8）式，我们进一步有\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\partial {\\rm CD}_k(\\tilde{p}, p_\\theta)\\over\\partial  w_{ij}} &= \\left[{1\\over T}\\sum\\limits_{t=1}^T\n",
    "\\left(\\sum\\limits_{h_i}p\\left(h_i|\\mathbf{v}^{(t)}\\right)h_iv_j^{(t)}\\right)\n",
    "\\right]-\\left[{1\\over T}\\sum\\limits_{t=1}^T\n",
    "\\left(\\sum\\limits_{h_i}p\\left(h_i|\\mathbf{u}^{(t)}\\right)h_iu_j^{(t)}\\right)\n",
    "\\right]\\equiv  \\left\\langle h_iv_j\\right\\rangle_{\\rm data} - \\left\\langle h_iv_j\\right\\rangle_{\\text{k-step data}}\\\\\n",
    "{\\partial {\\rm CD}_k(\\tilde{p}, p_\\theta)\\over\\partial b_{j}} &=\n",
    "\\left[{1\\over T}\\sum_{i=1}^Tv_j^{(t)}\\right]\n",
    "-\\left[{1\\over T}\\sum_{i=1}^Tu_j^{(t)}\\right]\\equiv  \\left\\langle v_j\\right\\rangle_{\\rm data} - \\left\\langle v_j\\right\\rangle_{\\text{k-step data}}\\\\\n",
    "{\\partial {\\rm CD}_k(\\tilde{p}, p_\\theta)\\over\\partial c_{i}} &=\\left[{1\\over T}\\sum\\limits_{i=1}^T\\left(\\sum\\limits_{h_i}p\\left(h_i|\\mathbf{v}^{(t)}\\right)h_i\\right)\\right]\n",
    "-\\left[{1\\over T}\\sum\\limits_{i=1}^T\\left(\\sum\\limits_{h_i}p\\left(h_i|\\mathbf{u}^{(t)}\\right)h_i\\right)\\right]\\equiv  \\left\\langle h_i\\right\\rangle_{\\rm data} - \\left\\langle h_i\\right\\rangle_{\\text{k-step data}}\\qquad\\text{(6.11)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "注意：**虽然理论上在 SGD 方法中，根据（6.11）式得到的参数值 和 （6.7）式得到的参数值不同，但在实际计算中，二者的差异很小，并且(6.11)式的计算效率提高很多，因此实际计算中一般采用（6.11）式进行 SGD 操作 **\n",
    "\n",
    "### 怎样使用一个训练好的 RBM\n",
    "\n",
    "当我们通过 SGD 方法，根据数据集 $\\mathcal{V}\\equiv \\left\\{\\mathbf{v}^{(1)},\\mathbf{v}^{(2)},\\cdots,\\mathbf{v}^{(T)}\\right\\}$ 将 RBM 参数调制最优后，可以如此“消费”：\n",
    "\n",
    "* 可以通过 RBM 生成新的 sample：随机初始化一组 $\\mathbf{\\Sigma=(V,H)}$ 值，通过 Gibbs sampling，迭代最够多步数后，得到一个新的sample\n",
    "\n",
    "\n",
    "* 根据公式 (5.4) 将 RBM 看做一个由 Visible nodes 向 hidden nodes 的前馈神经网络，输出由 公式（5.4）定义；原始的数据集  $\\mathcal{V}\\equiv \\left\\{\\mathbf{v}^{(1)},\\mathbf{v}^{(2)},\\cdots,\\mathbf{v}^{(T)}\\right\\}$ 经过这一层训练好的RBM后，得到一组新的“数据” $\\mathcal{H}\\equiv \\left\\{\\mathbf{h}^{(1)},\\mathbf{h}^{(2)},\\cdots,\\mathbf{h}^{(T)}\\right\\}$，我们利用这组新的数据，可以训练一个新的RBM，如此反复，我们可以逐层（所谓的“greedily train”）训练一个由RBM构成的深层神经网络，这种由RBM堆成的神经网络称为 “Deep belief network”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七、1D Ising model\n",
    "\n",
    "#### 4.1 1D Ising model (view of RG)\n",
    "\n",
    "\n",
    "给定如图所示的一个长度为$2N + 1$个 1D Ising model \n",
    "\n",
    "![Ising](../images/Ising1dRG.jpg)\n",
    "\n",
    "对应能量为： $$E(\\mathbf{\\sigma})=- K\\sum\\limits_{i=1}^{2N}\\left(\\sigma_i\\sigma_{i+1}\\right)  \\qquad\\qquad\\text{(7.1)}$$\n",
    "\n",
    "我们定义RG变换为：\n",
    "\n",
    "$$K(\\mathbf{s,\\sigma})=\\delta_{s_1\\;\\sigma_1}\\delta_{s_2\\;\\sigma_3}\\cdots\\delta_{s_{N+1}\\;\\sigma_{2N+1}}=\\prod\\limits_{i=1}^{N+1} \\delta_{s_i\\;\\sigma_{2i-1}},\\quad\\text{显然这个变换满足归一性条件 $\\forall \\mathbf{\\sigma}:\\;\\sum\\limits_\\mathbf{s}K(\\mathbf{s,\\sigma}) = 1 $}$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\exp[-\\tilde{E}(\\mathbf{s})]\\equiv\\sum\\limits_\\mathbf{\\sigma}K(\\mathbf{s,\\sigma}) \\exp\\left[-E(\\mathbf{\\sigma})\\right]&=&\\sum\\limits_\\mathbf{\\sigma_2\\;\\sigma_4\\cdots\\sigma_{2N}}\\exp\\left[ K\\sum\\limits_{i=1}^N\\left(\\sigma_{2i}s_i + \\sigma_{2i}s_{i+1}\\right)\\right]\\\\\n",
    "&=& \\sum\\limits_\\mathbf{\\sigma_2\\;\\sigma_4\\cdots\\sigma_{2N}}\\left(\\prod\\limits_{i=1}^N\\exp\\left[K\\left(\\sigma_{2i}s_i + \\sigma_{2i}s_{i+1}\\right)\\right]\\right)\\\\\n",
    "&=& \\prod\\limits_{i=1}^N \\left(\\sum\\limits_{\\sigma_{2i}\\;=\\pm1}\\exp\\left[K\\left(\\sigma_{2i}s_i + \\sigma_{2i}s_{i+1}\\right)\\right]\\right)\n",
    "\\end{matrix}$$\n",
    "\n",
    "\n",
    "由于\n",
    "$$\\sum\\limits_{\\sigma_{2i}\\;=\\pm1}\\exp\\left[K(\\sigma_{2i}s_i + \\sigma_{2i}s_{i+1})\\right]=\n",
    "\\left\\{\\begin{matrix}e^{2K} + e^{-2K}\\quad \\text{当$s_is_{i+1}=1$}\\\\\n",
    "2\\quad \\text{当$s_is_{i+1}=-1$}\\end{matrix}\\right\\}=2[\\cosh(2K)]^{1\\;\\over 2}[\\cosh(2K)]^{s_i\\;s_{i+1}\\;\\over2} = \\exp{\\left[ C^\\prime + K^\\prime s_is_{i+1}\\right]}\n",
    "$$ 其中 $$C^\\prime={1\\over2}\\ln\\left[4\\cosh(2K)\\right] ,\\quad K^\\prime = {1\\over2}\\ln\\left[(\\cosh(2K)\\right]$$\n",
    "\n",
    "因此上式进一步化为：\n",
    "\n",
    "$$\\exp[-\\tilde{E}(\\mathbf{s})] =\\prod\\limits_{i=1}^N  \\exp{\\left[ C^\\prime + K^\\prime s_is_{i+1}\\right]}= \\exp\\left[NC^\\prime + K^\\prime\\sum\\limits_{i=1}^N\\left(s_is_{i+1}\\right)\\right]$$\n",
    "\n",
    "即\n",
    "\n",
    "$$\\tilde{E}(\\mathbf{s})=-NC^\\prime - K^\\prime\\sum\\limits_{i=1}^N\\left(s_is_{i+1}\\right)$$\n",
    "\n",
    "\n",
    "#### 4.1 1d Ising model (view of RBM)\n",
    "\n",
    "上节的1d Ising model 总对应，如下图所示的 RBM （$N$ 个hidden variables, $N + 1$ 个 visible variables）：\n",
    "\n",
    "![Ising](../images/Ising1dRBM.jpg)\n",
    "\n",
    "对应能量为： $$E(\\mathbf{v,h})=- K\\sum\\limits_{i=1}^{N}\\left(v_ih_{i} + v_{i+1}h_i\\right)$$\n",
    "\n",
    "在RBM背景下，我们关心的计算量之一是 $p(\\mathbf{v})\\equiv\\sum\\limits_\\mathbf{h}p(\\mathbf{v,h})$：\n",
    "\n",
    "\n",
    "$$\\begin{matrix}\n",
    "p(\\mathbf{v})\\equiv \\sum\\limits_\\mathbf{h}p(\\mathbf{v,h})={1\\over Z}\\sum_\\mathbf{h}e^{-E(\\mathbf{v,h})}&=&{1\\over Z}\\sum\\limits_\\mathbf{h}\\exp\\left[ K\n",
    "\\sum\\limits_{i=1}^N\\left(v_ih_i + v_{i+1}h_{i}\\right)\\right]\\\\\n",
    "&=& {1\\over Z}\\prod\\limits_{i=1}^N \\left(\\sum\\limits_{h_i=\\pm1}\\exp\\left[K\\left(v_ih_i + v_{i+1}h_{i}\\right)\\right]\\right)\\\\\n",
    "&=& {1\\over Z}\\exp\\left[NC^\\prime + K^\\prime\\sum\\limits_{i=1}^N\\left(v_{i}v_{i+1}\\right)\\right] \\qquad\\qquad\\text{(7.2)}\n",
    "\\end{matrix}$$\n",
    "\n",
    "对比公式（7.1） 可见，二者的形式完全一致，因此可以将“renormalized energy”理解为 partial distribution $p(\\mathbf{v})$ （或$p(\\mathbf{h})$）对应的能量因子\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm\n",
    "\n",
    "In statistics, an expectation–maximization (EM) algorithm is an iterative method to find maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
