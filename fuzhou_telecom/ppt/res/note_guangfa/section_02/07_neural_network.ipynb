{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial_11: 深度学习与神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 基本原理\n",
    "\n",
    "> 问题：以图像识别为例，给定 $N$ 个样本 $\\{(x_1, y_1), \\cdots, (x_N, y_N)\\}$，其中$x_i$ 表示图像，$y_i$ 表示图像的类别标签（猫？狗？），现在给定一张新图 $x$, 识别出图像的标签\n",
    "\n",
    "一个前馈神经网络，描述一个条件概率函数$p(y|x,\\theta)$, 其中 $x$ 表述输入，\n",
    "\n",
    "$$ \n",
    "\\begin{align}\\theta^* &=\\mathop{\\rm argmax}\\limits_\\theta S(\\theta)\\\\\n",
    "S(\\theta) &\\equiv {1\\over n}\\sum_{i=1}^n \\log p(y_i|x_i,\\theta)\n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- logistic regression\n",
    "\n",
    "$$\\begin{align}\n",
    "z &= \\mathbf{x\\cdot w} + b\\\\\n",
    "a &= \\sigma(z)\\\\\n",
    "\\theta^* &= \\mathop{\\rm argmin}\\limits_\\theta \\sum_i\\|y_i - a_i\\|^2\n",
    "\\end{align}\\qquad \\text{(非概率模型)}\n",
    "$$ \n",
    "\n",
    "- percepton\n",
    "\n",
    "$$\\begin{align}\n",
    "z &= \\mathbf{x\\cdot w} + b\\\\\n",
    "a &=\\sigma(z)\\\\\n",
    "\\theta^* &= \\mathop{\\rm argmin}\\limits_\\theta\\sum_i\\left[- y_i\\ln a_i-(1-y_i)\\ln(1-a_i)\\right] \n",
    "\\end{align}\\qquad \\text{(概率模型)}\n",
    "$$ \n",
    "\n",
    "- neural network with 1-hidden layer (binary classification)\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{h} &= f(\\mathbf{xW_h} + \\mathbf{b_h})\\\\\n",
    "z &= \\mathbf{h\\cdot w_a} + b_a\\\\\n",
    "a&=\\sigma(z)\\\\\n",
    "\\theta^* &= \\mathop{\\rm argmin}\\limits_\\theta\\sum_i\\left[- y_i\\ln a_i-(1-y_i)\\ln(1-a_i)\\right] \n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "- neural network with 1-hidden layer (multiple classification)\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{h} &= f(\\mathbf{xW_h} + \\mathbf{b_h})\\\\\n",
    "\\mathbf{z} &= \\mathbf{h\\cdot W_a} + \\mathbf{b_a})\\\\\n",
    "a_k&={e^{z_k}\\over \\sum\\limits_j e^{z_j}}\\\\\n",
    "\\theta^* &= \\mathop{\\rm argmin}\\limits_\\theta\\sum_i\\left[- \\mathbf{y}_i\\cdot\\ln \\mathbf{a}_i\\right] \n",
    "\\end{align}\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SGD\n",
    "\n",
    "\n",
    "\n",
    "怎样求解如下形式的最优化问题\n",
    "$$\\theta^* = \\mathop{\\rm argmin}\\limits_\\theta S_\\theta,\\qquad S_\\theta\\equiv{1\\over N}\\sum_{i=1}^NL_\\theta(\\mathbf{x}_i,\\mathbf{y}_i)$$\n",
    "\n",
    "梯度算法的思路：\n",
    "$$\\theta \\leftarrow \\theta -\\eta\\nabla_\\theta S_\\theta$$\n",
    "\n",
    "但 $S_\\theta$ 由很多项求和构成，计算成本很高，于是引入 Stochastic 思想：在每步迭代中引入近似：\n",
    "\n",
    "$$S_\\theta = {1\\over N}\\sum_{i=1}^NL_\\theta(\\mathbf{x}_i,\\mathbf{y}_i)\\approx {1\\over M}\\sum_{i=1}^ML_\\theta(\\mathbf{x}_i,\\mathbf{y}_i)$$\n",
    "\n",
    "\n",
    "momentum, anealing learing rate and adaptive learning rate etc.\n",
    "\n",
    "[文章推荐](../papers/SGD_review.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 卷积神经网络\n",
    "\n",
    "**卷积操作**\n",
    "![conv2](images/conv2.jpg)\n",
    "![conv1](images/conv1.jpg)\n",
    "\n",
    "\n",
    "**卷积作用**\n",
    "![convl1](images/convl1.jpg)\n",
    "![convl2](images/convl2.jpg)\n",
    "![convl3](images/convl3.jpg)\n",
    "![convl4](images/convl4.jpg)\n",
    "![convl5](images/convl5.jpg)\n",
    "\n",
    "\n",
    "** 其他变种 **\n",
    "\n",
    "transpose cnn(fractional cnn) / dilated cnn\n",
    "\n",
    "http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Back propagation\n",
    "\n",
    "\n",
    "考虑如下过程：一个batch的samples （个数为$M$）输入一个前馈神经网络，并最终获得 cost function（“标量”函数 ）$\\mathscr{C}$ 的值,\n",
    "\n",
    "除输入层外，假设共有$L$层神经元，每层神经元的个数分别记为 $N_1,N_2,\\dots,N_L$;\n",
    "\n",
    "对于batch 中的第$m$个 sample （$m=1,2,\\dots,M$）,其对应第$l$层的第$i$个神经元输出（activation）记为 $a_{mi}^{(l)}$（$l=1,2,\\dots,L;\\quad i=1,2,\\dots,N_l$）\n",
    "\n",
    "将第$m$ 个sample 在第$l$ 层的所有输出记为$\\mathcal{A}_m^{(l)}$，即\n",
    "\n",
    "$$\\mathcal{A}_m^{(l)}\\equiv \\left\\{a_{m1}^{(l)},a_{m2}^{(l)},\\dots,a_{mN_l}^{(l)}\\right\\},\\qquad {\\rm size}\\left(\\mathcal{A}_m^{(l)}\\right)=N_l$$\n",
    "\n",
    "将batch中所有sample 在第$l$ 层的所有输出记为$\\mathcal{A}^{(l)}$，即\n",
    "$$\\mathcal{A}^{(l)}\\equiv \\mathcal{A}_1^{(l)} \\cup \\mathcal{A}_2^{(l)} \\cup\\dots\\cup\\mathcal{A}_M^{(l)} , \\qquad {\\rm size}\\left(\\mathcal{A}^{(l)}\\right)=M\\times N_l$$\n",
    "\n",
    "\n",
    "由于前馈神经网络中，每层输出值仅“直接”依赖于其上一层神经元的输出值，即，\n",
    "\n",
    "$$a_{mi}^{(l)} = f_i^{(l)}\\left(\\mathcal{A}_m^{(l-1)}, \\vec{\\theta}_i^{(l)}\\right),\\qquad\\text{（其中}m=1,2,\\dots,M\\text{分别对应M个sample）}$$\n",
    "\n",
    "其中$f_i^{(l)}$为，第$l$层第$i$个神经元的输出函数 (以$l-1$ 层神经元的输出作为其输入量)，$\\vec{\\theta}_i^{(l)}$ 是第$l$层第$i$个神经元的参数。\n",
    "\n",
    "cost function 仅“直接”依赖于最后一层输出值，即 $\\mathscr{C}=\\mathscr{C}\\left(\\mathcal{A}^{(L)}\\right)$，由于上述输出量的层层依赖关系，因此，总可以认为 $\\mathscr{C}=\\mathscr{C}\\left(\\mathcal{A}^{(l)}\\right),\\;\\forall l\\in\\{1,2,\\dots,L\\}$。记$l$层所有参数的集合为$\\Theta^{(l)}$， 即\n",
    "$\\Theta^{(l)}\\equiv\\left\\{\\vec{\\theta}_1^{(l)},\\vec{\\theta}_2^{(l)},\\dots,\\vec{\\theta}_{N_l}^{(l)}\\right\\}$；记神经网络中的所有参数为$\\Theta$，即\n",
    "$\\Theta\\equiv \\Theta^{(1)}\\cup \\Theta^{(2)}\\cup\\dots\\cup\\Theta^{(L)}$，则也可以认为 $\\mathscr{C}=\\mathscr{C}\\left(\\Theta\\right)$\n",
    "\n",
    "定义 $$\\delta_{mi}^{(l)}\\equiv {\\partial \\mathscr{C}\\left( \\mathcal{A}^{(l)} \\right) \\over \\partial a_{mi}^{(l)}}$$\n",
    "\n",
    "由于 $\\mathscr{C}$ 可以按batch中的各个sample拆分为：$\\mathscr{C}\\left(\\mathcal{A}^{(l)}\\right)=\\sum\\limits_{m=1}^M \\mathscr{C}_m\\left(\\mathcal{A}^{(l)}_m\\right)$，于是\n",
    "$$\\delta_{mi}^{(l)}= {\\partial \\mathscr{C}_m\\left( \\mathcal{A}^{(l)}_m \\right) \\over \\partial a_{mi}^{(l)}}$$\n",
    "\n",
    "我们有：\n",
    "\n",
    "$$\\delta_{mi}^{(L)} = {\\partial \\mathscr{C}\\left( \\mathcal{A}^{(L)} \\right) \\over \\partial a_{mi}^{(L)}}=\n",
    "{\\partial \\mathscr{C}_m\\left( \\mathcal{A}_m^{(L)} \\right) \\over \\partial a_{mi}^{(L)}}$$\n",
    "\n",
    "$$\\delta_{mi}^{(l-1)} = {\\partial \\mathscr{C}\\left( \\mathcal{A}^{(l-1)} \\right) \\over \\partial a_{mi}^{(l-1)}}=\\sum\\limits_{n=1}^M\n",
    "\\sum\\limits_{j=1}^{N_l}{\\partial \\mathscr{C}\\left( \\mathcal{A}^{(l)} \\right) \\over \\partial a_{nj}^{(l)}} \n",
    " {\\partial a_{nj}^{(l)}\\over \\partial a_{mi}^{(l-1)}} = \n",
    " \\sum\\limits_{j=1}^{N_l} \\delta_{mj}^{(l)} {\\partial f^{(l)}_j\\left(\\mathcal{A}_m^{(l-1)},\\vec{\\theta}_j^{(l)}\\right)\\over \\partial a_{mi}^{(l-1)}} \n",
    "$$\n",
    "\n",
    "$${\\partial \\mathscr{C}(\\Theta)\\over \\partial \\vec{\\theta}_i^{(l)}} = \\sum\\limits_{m=1}^M\\sum\\limits_{k=1}^L\\sum\\limits_{j=1}^{N_k}\n",
    "{\\partial \\mathscr{C}\\left( \\mathcal{A}^{(k)} \\right) \\over \\partial a_{mj}^{(k)}}{\\partial a_{mj}^{(k)}\\over  \\partial \\vec{\\theta}_i^{(l)}}=\\sum\\limits_{m=1}^{M}{\\partial \\mathscr{C}\\left( \\mathcal{A}^{(l)} \\right) \\over \\partial a_{mi}^{(l)}}\n",
    "{\\partial a_{mi}^{(l)}\\over \\partial\\vec{\\theta}_i^{(l)}}=\n",
    "\\sum\\limits_{m=1}^{M}\\delta_{mi}^{(l)}\n",
    "{\\partial f_i^{(l)}\\left(\\mathcal{A}_m^{(l-1)}, \\vec{\\theta}_i^{(l)}\\right) \\over \\partial\\vec{\\theta}_i^{(l)}} $$\n",
    "\n",
    "\n",
    "# max pool layer\n",
    "\n",
    "设由$l-1$层向$l$层的运算是一个 max pool 操作：对于batch 中的第$\\forall m$个 sample，第$l$层的第$\\forall i$个神经元，在第$l-1$ 层对应的“采样池”记为 ${\\rm pool}\\left(\\mathcal{A}_{m}^{(l-1)},i\\right)$，举个具体例子，\n",
    "\n",
    "假设$l$层的第7个神经元的输出值要求是$l-1$ 层的第2、3、5、8神经元输出值中的最大值，则${\\rm pool}\\left(\\mathcal{A}_{m}^{(l-1)},7\\right)\\equiv\\left\\{a_{m2}^{(l-1)},a_{m3}^{(l-1)},a_{m5}^{(l-1)},a_{m8}^{(l-1)}\\right\\},\\;a_{m7}^{(l)}=\\max\\left(\n",
    "\\left\\{a_{m2}^{(l-1)},a_{m3}^{(l-1)},a_{m5}^{(l-1)},a_{m8}^{(l-1)}\\right\\}\\right)\\equiv \\max{\\rm pool}\\left(\\mathcal{A}_{m}^{(l-1)},7\\right)$\n",
    "\n",
    "此时，第$l-1$层第$\\forall i$个神经元的导数为：\n",
    "$$\\delta_{mi}^{(l-1)} = \n",
    " \\sum\\limits_{j=1}^{N_l} \\delta_{mj}^{(l)} {\\partial \\max{\\rm pool}\\left(\\mathcal{A}_{m}^{(l-1)},j\\right)\\over \\partial a_{mi}^{(l-1)}}=\\sum\\limits_{[j]}\\delta_{mj}^{(l)} $$\n",
    " \n",
    "其中$\\sum\\limits_{[j]}$ 表示在$N_l$个“采样池”中仅对满足条件$\\;\\max{\\rm pool}\\left(\\mathcal{A}_{m}^{(l-1)},j\\right)=a_{mi}^{(l)}$ 所对应的角标$j$ 求和\n",
    "\n",
    "（注意：max pool 层只需要传递导数，无参数需要拟合）\n",
    "\n",
    "\n",
    "# linear kernel layer\n",
    "\n",
    "对于大多数前馈神经网络的中间层 $a_{mi}^{(l)} = f_i^{(l)}\\left(\\mathcal{A}_m^{(l-1)}, \\vec{\\theta}_i^{(l)}\\right)$ 中的$f_i^{(l)}$ 函数一致，且取如下linear kernel 形式：\n",
    "\n",
    "$$a_{mi}^{(l)} = F^{(l)}\\left(\\sum\\limits_{j=1}^{N_{l-1}}a_{mj}^{(l-1)}W_{ji}^{(l)} + b_i^{(l)}\\right)$$\n",
    "其中 $N_{l-1} + 1$ 个参数 $\\left\\{W_{1i}^{(l)},W_{2i}^{(l)},\\dots,W_{N_{l-1} i}^{(l)},b_i^{(l)}\\right\\}\\equiv\\vec{\\theta}_i^{(l)}$，令$z_{mi}^{(l)}=\\sum\\limits_{j}a_{mj}^{(l-1)}W_{ji}^{(l)} + b_i^{(l)}$，并将$l$层所有参数的集合$\\Theta^{(l)}$ 在linear kernel 背景下改用$\\left(\\mathbf{W}^{(l)}，\\mathbf{b}^{(l)}\\right)$ 标记，并将$\\Theta$ 中 除$l$层以外的所有参数集记为$\\Theta'$，我们有：\n",
    "\n",
    "$$\\delta_{mi}^{(l-1)} = \n",
    " \\sum\\limits_{j=1}^{N_l} \\delta_{mj}^{(l)} \\left({\\partial F^{(l)}\\left(z\\right) \\over \\partial z}\\right)_{z=z_{mj}^{(l)}}W_{ij}^{(l)}\n",
    "$$\n",
    "\n",
    "$${\\partial \\mathscr{C}\\left(\\Theta',\\mathbf{W}^{(l)},\\mathbf{b}^{(l)}\\right)\\over \\partial W_{ji}^{(l)}}=\\sum\\limits_{m=1}^{M}\\delta_{mi}^{(l)}\n",
    "\\left({\\partial F^{(l)}\\left(z\\right) \\over \\partial z}\\right)_{z=z_{mi}^{(l)}}a_{mj}^{(l-1)}$$\n",
    "\n",
    "$${\\partial \\mathscr{C}\\left(\\Theta',\\mathbf{W}^{(l)},\\mathbf{b}^{(l)}\\right)\\over \\partial b_{i}^{(l)}}=\\sum\\limits_{m=1}^{M}\\delta_{mi}^{(l)}\n",
    "\\left({\\partial F^{(l)}\\left(z\\right) \\over \\partial z}\\right)_{z=z_{mi}^{(l)}}$$\n",
    "\n",
    "注意：$\\langle ji\\rangle$ 可遍历网络Graph中$l-1$和$l$层间的所有连线；（每一根连线，对应一个weight参数$W_{ji}^{(l)}$。对于全连通网络，$i$遍历所有$l$层的$N_l$个神经元，$j$ 遍历$l-1$层的$N_{l-1}$个神经元；）\n",
    "\n",
    "特别地：\n",
    "\n",
    "一、 当$F^{(l)}$ 取 ReLU neuron形式时，则\n",
    "\n",
    "$$\\left({\\partial F^{(l)}\\left(z\\right) \\over \\partial z}\\right)_{z=z_{mi}^{(l)}}=\\left\\{\n",
    "\\begin{matrix}1,\\quad\\text{if } z_{mi}^{(l)}\\ge0\\\\\n",
    "0,\\quad\\text{if } z_{mi}^{(l)}<0\n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "$${\\partial \\mathscr{C}(\\Theta)\\over \\partial \\omega_{\\alpha s}^{(l)}}=\\sum\\limits_{\\langle ij\\rangle_{\\alpha s}}{\\partial \\mathscr{C}(\\Theta)\\over \\partial W_{ji}^{(l)}}$$\n",
    "\n",
    "二、对于卷积神经网络，由于weights share，独立的参数比Graph中的连线（对应weights）和节点（对应bias）更少。设 $l-1$层向$l$层的运算是一个“卷积”操作，并且我们将独立的“卷积”weight 和 bias参数分别记为$\\omega_{\\mu\\nu}^{(l)}$ 和 $\\beta_{\\nu}^{(l)}$ (其中$\\nu$ 标记feature map，$\\mu$ 标记该feature map上的各weight分量，例如：若$l$-th layer有10个feature map，每个feature map有25个独立weight，则独立的参数共有$10\\times25 + 10=260$个)，将这些独立的weight和bias参数集合记为，$\\left(\\Omega^{(l)},B^{(l)}\\right)$,则 cost function的依赖参数可以看做：$\\mathscr{C}=\\mathscr{C}\\left(\\Theta',\\Omega^{(l)},B^{(l)}\\right)$，且我们有，\n",
    "\n",
    "$$\\delta_{mi}^{(l-1)} = \n",
    " \\sum\\limits_{j=1}^{N_l} \\delta_{mj}^{(l)} \\left({\\partial F^{(l)}\\left(z\\right) \\over \\partial z}\\right)_{z=z_{mj}^{(l)}}W_{ij}^{(l)}\n",
    "$$\n",
    "\n",
    "$${\\partial \\mathscr{C}\\left(\\Theta',\\Omega^{(l)},B^{(l)}\\right)\\over \\partial \\omega_{\\mu\\nu}^{(l)}}=\n",
    "\\sum\\limits_{ij}{\\partial \\mathscr{C}\\left(\\Theta',\\mathbf{W}^{(l)},\\mathbf{b}^{(l)}\\right)\\over \\partial W_{ji}^{(l)}}\n",
    "{\\partial W_{ji}^{(l)}\\over\\partial\\omega_{\\mu\\nu}^{(l)}}=\\sum\\limits_{\\langle ji\\rangle_{\\mu\\nu}}\\;\\sum\\limits_{m=1}^{M}\\delta_{mi}^{(l)}\n",
    "\\left({\\partial F^{(l)}\\left(z\\right) \\over \\partial z}\\right)_{z=z_{mi}^{(l)}}a_{mj}^{(l-1)}$$\n",
    "\n",
    "$$\n",
    "{\\partial \\mathscr{C}\\left(\\Theta',\\Omega^{(l)},B^{(l)}\\right)\\over \\partial \\beta_{\\nu}^{(l)}} =\n",
    "\\sum\\limits_{i}{\\partial \\mathscr{C}\\left(\\Theta',\\mathbf{W}^{(l)},\\mathbf{b}^{(l)}\\right)\\over \\partial b_{i}^{(l)}}{\n",
    "\\partial b_{i}^{(l)}\\over \\partial \\beta_{\\nu}^{(l)}}=\\sum\\limits_{\\langle i\\rangle_\\nu}\\sum\\limits_{m=1}^{M}\\delta_{mi}^{(l)}\n",
    "\\left({\\partial F^{(l)}\\left(z\\right) \\over \\partial z}\\right)_{z=z_{mi}^{(l)}}$$\n",
    "\n",
    "其中$=\\sum\\limits_{\\langle ji\\rangle_{\\mu\\nu}}$ 表示对Graph中$l-1$层与$l$层之间所有“卷积”连线$\\langle ji\\rangle$求和（连线对应的卷积weight 值要求是 $\\omega_{\\mu\\nu}^{(l)}$）；$\\sum\\limits_{\\langle i\\rangle_\\nu}$ 表示对第$l$层上第$\\mu$个feature map上的所有节点求和\n",
    "\n",
    "我们再以tensoflow中的卷积函数来做比较：\n",
    "\n",
    "<strong style=\"color:#f0f\">output</strong> = tf.nn.conv2d(<strong style=\"color:green\">input</strong>, <strong style=\"color:red\">filter</strong>, <strong style=\"color:blue\">strides</strong>, <strong style=\"color:blue\">padding</strong>, use_cudnn_on_gpu=None, <strong style=\"color:green\">data_format</strong>=None, name=None)\n",
    "\n",
    "* <strong style=\"color:green\">input</strong>： 对应输入层$a^{(l-1)}_{mj}$，对于输入层的节点index $j$ 可以用其在图像中的行列位置及所属channel来表征（$l-1$层是原始图像输入时，有rgb 三个 channel；$l-1$层是中间层时，每个channel对应一个feature map）；因此 input 可以看做是一个由 $a^{(l-1)}_{mj}$ reshape 而成的4阶tensor：\n",
    "\n",
    "\n",
    "<strong style=\"color:green\">shape(input) = [batch_size, input_height, input_width, input_channels(depth)]</strong>\n",
    "\n",
    "（注意：<strong style=\"color:green\">data_format</strong> 字符串指定 input 的格式，默认为\"NHWC\", 表示input的脚标依次表示batch number，height，width，channels；指定为\"NWHC\" 时，则input的脚标依次表示batch number，width，height，channels；此时：\n",
    "\n",
    "<strong style=\"color:green\">shape(input) = [batch_size, input_width, input_height, input_channels(depth)]</strong>）\n",
    "\n",
    "* <strong style=\"color:red\">filter</strong>：对应独立参数 $\\omega_{\\mu\\nu}^{(l)}$，$\\nu$ 表示输出到哪个feature map（又称为output channel）；在卷积操作中，“输入图像”被划分为各个小的patch，这些patch上每个位置对应一个weight参数，因此$\\mu$ 可以用patch上对应的行列位置及所在的channel来表征；因此，filter 可以看作是一个由  $\\omega_{\\mu\\nu}^{(l)}$ reshape 而成的四阶tensor：\n",
    "\n",
    "<strong style=\"color:red\">shape(filter) = [filter_height, filter_width, input_channels(depth), output_channels]</strong>\n",
    "\n",
    "由于一般图像在xy方向上应该同等对待，因此一般取 filter_height = filter_width\n",
    "\n",
    "*  <strong style=\"color:blue\">strides</strong>：描述怎样在“输入图像”上选取参与卷积的各个patch,即描述patch平移的步长:\n",
    "\n",
    "<strong style=\"color:blue\">strides = [batch_step, height_step, width_step, channel_step]</strong>\n",
    "\n",
    "由于我们需要遍历所有batch和所有channel，所以一般<strong style=\"color:blue\">选定 batch_step=1，channel_step=1</strong>；另外由于一般图像在xy方向上应该同等对待，因此一般取 height_step = width_step \n",
    "\n",
    "* <strong style=\"color:blue\">padding：</strong>同样描述怎样在“输入图像”上选取参与卷积的各个patch，当padding 取“SAME”字符串时，将“输入图像”的四周padding 0 元素（延拓图像的高宽），以确保patch 在xy方向上可平移的步数，与原图像的pixel数一致，即 output_height = input_height, output_width = input_width；padding 取“VALID”字符串时，不用在“输入图像”的四周padding 0 元素，此时\n",
    "\n",
    "$${\\rm output\\_width} = {\\rm input\\_width -  filter\\_width\\over width\\_step} + 1$$\n",
    "$${\\rm output\\_height} = {\\rm input\\_height -  filter\\_height\\over height\\_step} + 1$$\n",
    "\n",
    "\n",
    "结合上面的公式，可以认为<strong style=\"color:blue\">strides 和 padding 参数是用来描述 $W_{ji}^{(l)}$ 和 $\\omega_{\\mu\\nu}^{(l)}$ 之间对应关系的</strong>\n",
    "\n",
    "* <strong style=\"color:#f0f\">output</strong>： 对应输出层$a^{(l)}_{mi}$，和input参数类似，output可以看做是一个由 $a^{(l)}_{mi}$ reshape 而成的4阶tensor：\n",
    "\n",
    "\n",
    "<strong style=\"color:#f0f\">shape(output) = [batch_size, output_height, output_width, output_channels(depth)]</strong>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
